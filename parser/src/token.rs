use std::cmp::Ordering;

use crate::internal::*;

#[derive(Debug, PartialEq, Eq, Clone, Copy, Default)]
pub enum TokenKind {
  Ampersand,
  Backtick,
  Backslash,
  Bang,
  BeginInclude,
  CalloutNumber,
  Caret,
  CloseBrace,
  CloseBracket,
  Colon,
  Comma,
  Dashes,
  DelimiterLine,
  Digits,
  Directive,
  Discard,
  DoubleQuote,
  Dots,
  EndInclude,
  EqualSigns,
  #[default]
  Eof,
  ForwardSlashes,
  GreaterThan,
  Hash,
  LessThan,
  MacroName,
  MaybeEmail,
  Newline,
  OpenBrace,
  OpenBracket,
  Percent,
  Pipe,
  Plus,
  SemiColon,
  SingleQuote,
  Star,
  TermDelimiter,
  Tilde,
  Underscore,
  Whitespace,
  Word,
}

#[derive(Clone, PartialEq, Eq)]
pub struct Token<'arena> {
  pub kind: TokenKind,
  pub loc: SourceLocation,
  pub lexeme: BumpString<'arena>,
}

impl<'arena> Token<'arena> {
  pub fn new(kind: TokenKind, loc: impl Into<SourceLocation>, lexeme: BumpString<'arena>) -> Self {
    Self { kind, loc: loc.into(), lexeme }
  }

  pub fn into_source_string(self) -> SourceString<'arena> {
    SourceString::new(self.lexeme, self.loc)
  }

  pub fn to_url_scheme(&self) -> Option<UrlScheme> {
    match self.kind {
      TokenKind::MacroName => match self.lexeme.as_str() {
        "https:" => Some(UrlScheme::Https),
        "http:" => Some(UrlScheme::Http),
        "ftp:" => Some(UrlScheme::Ftp),
        "irc:" => Some(UrlScheme::Irc),
        "mailto:" => Some(UrlScheme::Mailto),
        _ => None,
      },
      _ => None,
    }
  }

  pub fn len(&self) -> usize {
    self.lexeme.len()
  }

  pub fn parse_callout_num(&self) -> Option<u8> {
    let ascii_digits = self
      .lexeme
      .bytes()
      .filter(u8::is_ascii_digit)
      .collect::<SmallVec<[u8; 3]>>();
    if ascii_digits.is_empty() {
      None // autogenerated: <.>
    } else {
      // SAFETY: we only have ascii digits, so this is fine
      let num_str = unsafe { std::str::from_utf8_unchecked(&ascii_digits) };
      // maybe better warn than expect?
      Some(num_str.parse::<u8>().expect("exceeded max 255 callouts"))
    }
  }

  pub fn drop_leading_bytes(&mut self, n: u32) {
    if n == 0 {
      return;
    }
    debug_assert!(n as usize <= self.lexeme.len());
    self.kind = TokenKind::Word;
    let mut removed = 0;
    loop {
      let char = self.lexeme.remove(0);
      let mut buf = [0; 4];
      let bytes = char.encode_utf8(&mut buf).as_bytes();
      removed += bytes.len();
      match removed.cmp(&(n as usize)) {
        Ordering::Less => continue,
        Ordering::Equal => break,
        Ordering::Greater => panic!("Token::drop_leading_bytes() mid-char boundary"),
      }
    }
    self.loc.start += n;
  }
}

pub trait TokenIs {
  fn is_url_scheme(&self) -> bool;
  fn is(&self, kind: TokenKind) -> bool;
  fn is_len(&self, kind: TokenKind, len: usize) -> bool;
  fn is_not(&self, kind: TokenKind) -> bool {
    !self.is(kind)
  }
  fn is_not_len(&self, kind: TokenKind, len: usize) -> bool {
    !self.is_len(kind, len)
  }
  fn is_whitespaceish(&self) -> bool {
    self.is(TokenKind::Whitespace) || self.is(TokenKind::Newline)
  }
}

impl<'arena> DefaultIn<'arena> for Token<'arena> {
  fn default_in(bump: &'arena Bump) -> Self {
    Self {
      kind: TokenKind::Eof,
      loc: SourceLocation::default(),
      lexeme: BumpString::from_str_in("", bump),
    }
  }
}

impl<'arena> TokenIs for Token<'arena> {
  fn is(&self, kind: TokenKind) -> bool {
    self.kind == kind
  }

  fn is_len(&self, kind: TokenKind, len: usize) -> bool {
    self.kind == kind && self.len() == len
  }

  fn is_url_scheme(&self) -> bool {
    self.to_url_scheme().is_some()
  }
}

impl<'arena> TokenIs for Option<&Token<'arena>> {
  fn is(&self, kind: TokenKind) -> bool {
    self.map_or(false, |t| t.is(kind))
  }

  fn is_len(&self, kind: TokenKind, len: usize) -> bool {
    self.map_or(false, |t| t.is_len(kind, len))
  }

  fn is_url_scheme(&self) -> bool {
    self.map_or(false, |t| t.is_url_scheme())
  }
}

impl<'arena> std::fmt::Debug for Token<'arena> {
  fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
    let lexeme = if &self.lexeme == "\n" {
      "\\n".to_string()
    } else {
      self.lexeme.to_string()
    };
    write!(
      f,
      "Token {{ {:?}, \"{}\", {:?} }}",
      self.kind, lexeme, self.loc
    )
  }
}
